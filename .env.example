# LLM Provider Configuration
# Set LLM_PROVIDER to "ollama" (default) or "together"
LLM_PROVIDER=ollama

# Ollama Configuration (default provider)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

# Together AI Configuration
TOGETHER_API_KEY=your_together_api_key_here
TOGETHER_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo

# Optional: Helicone for analytics (works with Together AI)
HELICONE_API_KEY=your_helicone_api_key_here

# Optional: Upstash Redis for rate limiting
UPSTASH_REDIS_REST_URL=your_upstash_redis_url_here
UPSTASH_REDIS_REST_TOKEN=your_upstash_redis_token_here